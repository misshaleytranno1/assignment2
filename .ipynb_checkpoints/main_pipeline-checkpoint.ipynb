{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a467b038-0f3f-43b9-b968-1d86fdd39a5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "\nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Users/haleytran/Downloads/MLE_A2-main/env_asm2/lib/python3.11/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\\n  Referenced from: <B111F8D5-6AC6-3245-A6B5-94693F6992AB> /Users/haleytran/Downloads/MLE_A2-main/env_asm2/lib/python3.11/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: tried: '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file)\"]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mXGBoostError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclassification\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BinaryClassificationEvaluator\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/MLE_A2-main/env_asm2/lib/python3.11/site-packages/xgboost/__init__.py:6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"XGBoost: eXtreme Gradient Boosting library.\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[33;03mContributors: https://github.com/dmlc/xgboost/blob/master/CONTRIBUTORS.md\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tracker  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m collective\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     Booster,\n\u001b[32m     10\u001b[39m     DataIter,\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m     build_info,\n\u001b[32m     16\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/MLE_A2-main/env_asm2/lib/python3.11/site-packages/xgboost/tracker.py:9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01menum\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IntEnum, unique\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, Optional, Union\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _LIB, _check_call, _deprecate_positional_args, make_jcargs\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_family\u001b[39m(addr: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m     13\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get network family from address.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/MLE_A2-main/env_asm2/lib/python3.11/site-packages/xgboost/core.py:308\u001b[39m\n\u001b[32m    304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\n\u001b[32m    307\u001b[39m \u001b[38;5;66;03m# load the XGBoost library globally\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m _LIB = \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check_call\u001b[39m(ret: \u001b[38;5;28mint\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[32m    313\u001b[39m \n\u001b[32m    314\u001b[39m \u001b[33;03m    This function will raise exception when error occurs.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    320\u001b[39m \u001b[33;03m        return value from API calls\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/MLE_A2-main/env_asm2/lib/python3.11/site-packages/xgboost/core.py:270\u001b[39m, in \u001b[36m_load_lib\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    268\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib_success:\n\u001b[32m    269\u001b[39m         libname = os.path.basename(lib_paths[\u001b[32m0\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(\n\u001b[32m    271\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    272\u001b[39m \u001b[33mXGBoost Library (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlibname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) could not be loaded.\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[33mLikely causes:\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[33m  * OpenMP runtime is not installed\u001b[39m\n\u001b[32m    275\u001b[39m \u001b[33m    - vcomp140.dll or libgomp-1.dll for Windows\u001b[39m\n\u001b[32m    276\u001b[39m \u001b[33m    - libomp.dylib for Mac OSX\u001b[39m\n\u001b[32m    277\u001b[39m \u001b[33m    - libgomp.so for Linux and other UNIX-like OSes\u001b[39m\n\u001b[32m    278\u001b[39m \u001b[33m    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\u001b[39m\n\u001b[32m    279\u001b[39m \n\u001b[32m    280\u001b[39m \u001b[33m  * You are running 32-bit Python on a 64-bit OS\u001b[39m\n\u001b[32m    281\u001b[39m \n\u001b[32m    282\u001b[39m \u001b[33mError message(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos_error_list\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m    284\u001b[39m         )\n\u001b[32m    285\u001b[39m     _register_log_callback(lib)\n\u001b[32m    287\u001b[39m     libver = _lib_version(lib)\n",
      "\u001b[31mXGBoostError\u001b[39m: \nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Users/haleytran/Downloads/MLE_A2-main/env_asm2/lib/python3.11/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\\n  Referenced from: <B111F8D5-6AC6-3245-A6B5-94693F6992AB> /Users/haleytran/Downloads/MLE_A2-main/env_asm2/lib/python3.11/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: tried: '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file)\"]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, concat_ws, date_format\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171bf6ae-a5df-4828-b60f-969a6e97cf57",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f4e889-43df-4367-a43e-fc50f5f32fdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"setup\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load gold layer\n",
    "features = spark.read.parquet(\"data/gold/feature_store\")\n",
    "labels   = spark.read.parquet(\"data/gold/label_store\")\n",
    "\n",
    "# Construct loan_id in features\n",
    "features = features.withColumn(\n",
    "    \"loan_id\",\n",
    "    concat_ws(\"_\", col(\"Customer_ID\"), date_format(col(\"feature_snapshot_date\"), \"yyyy_MM_dd\"))\n",
    ")\n",
    "\n",
    "# Join labels\n",
    "labeled_df = features.join(\n",
    "    labels.select(\"loan_id\", \"label\"),\n",
    "    on=\"loan_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Filter labeled only\n",
    "combined_labeled = labeled_df.filter(col(\"label\").isNotNull())\n",
    "\n",
    "# Save to datamart\n",
    "combined_labeled.write.mode(\"overwrite\").parquet(\"data/gold/combined_labeled.parquet\")\n",
    "\n",
    "print(\"Setup complete: combined_labeled.parquet written.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538e1a2f-e7fe-47f6-8322-b0de71451e42",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56df73f8-4550-4496-a2f8-2bb436a22470",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load prepared labeled dataset\n",
    "df = spark.read.parquet(\"data/gold/combined_labeled.parquet\")\n",
    "\n",
    "# Assemble and scale features\n",
    "non_features = ['Customer_ID', 'feature_snapshot_date', 'loan_id', 'label']\n",
    "vec_cols = [c for c in df.columns if c.endswith(\"_vec\")]\n",
    "feature_cols = [c for c in df.columns if c not in non_features + vec_cols]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"raw_features\")\n",
    "assembled = assembler.transform(df)\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"raw_features\", outputCol=\"features\", withStd=True, withMean=True)\n",
    "scaler_model = scaler.fit(assembled)\n",
    "scaled = scaler_model.transform(assembled)\n",
    "\n",
    "# Split by time\n",
    "train_df = scaled.filter((col(\"feature_snapshot_date\") >= \"2023-01-01\") & (col(\"feature_snapshot_date\") <= \"2023-12-01\"))\n",
    "val_df   = scaled.filter(col(\"feature_snapshot_date\") == \"2024-01-01\")\n",
    "test_df  = scaled.filter(col(\"feature_snapshot_date\") == \"2024-02-01\")\n",
    "\n",
    "# Train logistic regression\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=100)\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(predictions):\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"probability\", metricName=\"areaUnderROC\")\n",
    "    auc = evaluator.evaluate(predictions)\n",
    "\n",
    "    preds = predictions.withColumn(\"correct\", when(col(\"prediction\") == col(\"label\"), 1).otherwise(0))\n",
    "    total = preds.count()\n",
    "    correct = preds.filter(col(\"correct\") == 1).count()\n",
    "    tp = preds.filter((col(\"prediction\") == 1) & (col(\"label\") == 1)).count()\n",
    "    fp = preds.filter((col(\"prediction\") == 1) & (col(\"label\") == 0)).count()\n",
    "    fn = preds.filter((col(\"prediction\") == 0) & (col(\"label\") == 1)).count()\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall    = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1        = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"AUC\": round(auc, 4),\n",
    "        \"Accuracy\": round(accuracy, 4),\n",
    "        \"Precision\": round(precision, 4),\n",
    "        \"Recall\": round(recall, 4),\n",
    "        \"F1 Score\": round(f1, 4)\n",
    "    }\n",
    "\n",
    "# Create folder for metrics\n",
    "metrics_dir = \"model_store/logreg_metrics\"\n",
    "os.makedirs(metrics_dir, exist_ok=True)\n",
    "\n",
    "# Evaluate on splits\n",
    "for name, split in [(\"train\", train_df), (\"validation\", val_df), (\"test\", test_df)]:\n",
    "    pred = lr_model.transform(split)\n",
    "    metrics = evaluate_model(pred)\n",
    "    print(f\"\\n=== {name.capitalize()} Metrics ===\")\n",
    "    print(metrics)\n",
    "    \n",
    "    # Save to corresponding JSON\n",
    "    metrics_path = os.path.join(metrics_dir, f\"logreg_{name}.json\")\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    print(f\"{name.capitalize()} metrics saved to {metrics_path}\")\n",
    "\n",
    "# Save model (if needed later in selection step)\n",
    "model_path = \"model_store/logistic_model\"\n",
    "if os.path.exists(model_path):\n",
    "    import shutil\n",
    "    shutil.rmtree(model_path)\n",
    "lr_model.save(model_path)\n",
    "print(\"\\nLogistic regression model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543b6793-7053-4658-966a-9423224cda75",
   "metadata": {
    "tags": []
   },
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c24cb3-5ebd-4ae0-bc1e-fe54a73a6188",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper: convert Spark DataFrame to NumPy\n",
    "def spark_df_to_numpy(df):\n",
    "    pdf = df.select(\"features\", \"label\").toPandas()\n",
    "    X = np.vstack(pdf[\"features\"].values)\n",
    "    y = pdf[\"label\"].values\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = spark_df_to_numpy(train_df)\n",
    "X_val, y_val     = spark_df_to_numpy(val_df)\n",
    "X_test, y_test   = spark_df_to_numpy(test_df)\n",
    "\n",
    "# Helper: evaluation\n",
    "def evaluate_model(y_true, y_pred, y_prob):\n",
    "    return {\n",
    "        \"AUC\":       round(roc_auc_score(y_true, y_prob), 4),\n",
    "        \"Accuracy\":  round(accuracy_score(y_true, y_pred), 4),\n",
    "        \"Precision\": round(precision_score(y_true, y_pred), 4),\n",
    "        \"Recall\":    round(recall_score(y_true, y_pred), 4),\n",
    "        \"F1 Score\":  round(f1_score(y_true, y_pred), 4)\n",
    "    }\n",
    "\n",
    "# Hyperparameter tuning\n",
    "best_model = None\n",
    "best_auc = 0\n",
    "best_n = None\n",
    "\n",
    "for n in [50, 100, 200]:\n",
    "    model = XGBClassifier(n_estimators=n, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_val_prob = model.predict_proba(X_val)[:, 1]\n",
    "    auc = roc_auc_score(y_val, y_val_prob)\n",
    "    print(f\"n={n} â†’ Val AUC: {auc:.4f}\")\n",
    "\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        best_model = model\n",
    "        best_n = n\n",
    "\n",
    "print(f\"\\nBest XGBoost n_estimators: {best_n}, Validation AUC: {best_auc:.4f}\")\n",
    "\n",
    "# Evaluate on all sets\n",
    "results = {}\n",
    "for name, X, y in [(\"train\", X_train, y_train), (\"val\", X_val, y_val), (\"test\", X_test, y_test)]:\n",
    "    y_pred = best_model.predict(X)\n",
    "    y_prob = best_model.predict_proba(X)[:, 1]\n",
    "    results[name] = evaluate_model(y, y_pred, y_prob)\n",
    "\n",
    "# Print & save metrics\n",
    "metrics_dir = \"model_store/xgboost_metrics\"\n",
    "os.makedirs(metrics_dir, exist_ok=True)\n",
    "\n",
    "for name, metrics in results.items():\n",
    "    print(f\"\\n=== {name.capitalize()} Metrics ===\")\n",
    "    print(metrics)\n",
    "\n",
    "    metrics_path = os.path.join(metrics_dir, f\"xgboost_{name}.json\")\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    print(f\"{name.capitalize()} metrics saved to {metrics_path}\")\n",
    "\n",
    "# Save model\n",
    "os.makedirs(\"model_store\", exist_ok=True)\n",
    "joblib.dump(best_model, \"model_store/xgboost_model.pkl\")\n",
    "print(\"XGBoost model saved to model_store/xgboost_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d55dcd8-0373-4ec0-8c4e-3416ef14aa91",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Select Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18584040-8bbc-4f54-931c-de2e09067123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load test AUCs from saved metrics\n",
    "with open(\"model_store/logreg_metrics/logreg_test.json\") as f:\n",
    "    logreg_metrics = json.load(f)\n",
    "with open(\"model_store/xgboost_metrics/xgboost_test.json\") as f:\n",
    "    xgb_metrics = json.load(f)\n",
    "\n",
    "logreg_auc = logreg_metrics[\"AUC\"]\n",
    "xgb_auc = xgb_metrics[\"AUC\"]\n",
    "\n",
    "print(\"Validation AUCs:\")\n",
    "print(f\"Logistic Regression: {logreg_auc}\")\n",
    "print(f\"XGBoost: {xgb_auc}\")\n",
    "\n",
    "# Select best model\n",
    "if xgb_auc > logreg_auc:\n",
    "    best_model = joblib.load(\"model_store/xgboost_model.pkl\")\n",
    "    joblib.dump(best_model, \"model_store/best_model.pkl\")\n",
    "    print(\"\\nXGBoost selected as best model and saved as best_model.pkl\")\n",
    "else:\n",
    "    print(\"\\nLogistic Regression has better AUC but cannot be saved with joblib.\")\n",
    "    print(\"Use PySpark's .load() from model_store/logistic_model when needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f709461-a493-4f46-9e5f-83588c6ca5f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### OOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dd8e77-c79f-4a33-af8c-e08c08e013b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load best model (assumed XGBoost)\n",
    "model = joblib.load(\"model_store/best_model.pkl\")\n",
    "\n",
    "# Helper to convert Spark â†’ NumPy\n",
    "def spark_df_to_numpy(df):\n",
    "    pdf = df.select(\"loan_id\", \"features\").toPandas()\n",
    "    loan_ids = pdf[\"loan_id\"].values\n",
    "    X = np.vstack(pdf[\"features\"].values)\n",
    "    return loan_ids, X\n",
    "\n",
    "# Loop over OOT months\n",
    "oot_months = [\"2024-03-01\", \"2024-04-01\", \"2024-05-01\", \"2024-06-01\"]\n",
    "\n",
    "for month in oot_months:\n",
    "    df_month = scaled.filter(col(\"feature_snapshot_date\") == month)\n",
    "    loan_ids, X = spark_df_to_numpy(df_month)\n",
    "\n",
    "    # Predict probabilities and labels\n",
    "    y_prob = model.predict_proba(X)[:, 1]\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    # Create output DataFrame\n",
    "    pdf_out = {\n",
    "        \"loan_id\": loan_ids,\n",
    "        \"prediction\": y_pred,\n",
    "        \"probability\": y_prob,\n",
    "        \"snapshot_date\": [month] * len(loan_ids)\n",
    "    }\n",
    "\n",
    "    import pandas as pd\n",
    "    spark_out = spark.createDataFrame(pd.DataFrame(pdf_out))\n",
    "\n",
    "    # Save to gold datamart\n",
    "    output_path = f\"datamart/gold/predictions/predictions_oot_{month}.parquet\"\n",
    "    spark_out.write.mode(\"overwrite\").parquet(output_path)\n",
    "    print(f\"âœ… Saved predictions for {month} to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35314b9e-0454-4bb4-9eea-f36e2d59ecc1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Monitor Stability (PSI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0171a0-4c3d-439d-a6b6-73e2cfa26854",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reference = February 2024\n",
    "ref_df = df.filter(col(\"feature_snapshot_date\") == \"2024-02-01\").toPandas()\n",
    "\n",
    "# Feature columns\n",
    "non_features = ['Customer_ID', 'feature_snapshot_date', 'loan_id', 'label']\n",
    "vec_cols = [c for c in ref_df.columns if c.endswith(\"_vec\")]\n",
    "feature_cols = [c for c in ref_df.columns if c not in non_features + vec_cols]\n",
    "\n",
    "# PSI calculator\n",
    "def calculate_psi(expected, actual, buckets=10):\n",
    "    breakpoints = np.percentile(expected, np.linspace(0, 100, buckets + 1))\n",
    "    expected_bins = np.histogram(expected, bins=breakpoints)[0] / len(expected)\n",
    "    actual_bins = np.histogram(actual, bins=breakpoints)[0] / len(actual)\n",
    "    psi = np.sum((expected_bins - actual_bins) * np.log((expected_bins + 1e-6) / (actual_bins + 1e-6)))\n",
    "    return round(psi, 4)\n",
    "\n",
    "# Loop through OOT months\n",
    "monitor_months = [\"2024-03-01\", \"2024-04-01\", \"2024-05-01\", \"2024-06-01\"]\n",
    "psi_results = {}\n",
    "\n",
    "for month in monitor_months:\n",
    "    current_df = df.filter(col(\"feature_snapshot_date\") == month).toPandas()\n",
    "    psi_scores = {}\n",
    "\n",
    "    for feat in feature_cols:\n",
    "        try:\n",
    "            psi_scores[feat] = calculate_psi(ref_df[feat].dropna(), current_df[feat].dropna())\n",
    "        except Exception:\n",
    "            psi_scores[feat] = None\n",
    "\n",
    "    # Valid PSI scores only\n",
    "    valid_psi = {k: v for k, v in psi_scores.items() if v is not None}\n",
    "    avg_psi = round(np.mean(list(valid_psi.values())), 4)\n",
    "    max_psi = round(max(valid_psi.values()), 4)\n",
    "\n",
    "    # Alerting\n",
    "    drifted_feats = [k for k, v in valid_psi.items() if v > 0.1]\n",
    "\n",
    "    if avg_psi > 0.25:\n",
    "        alert = \"ðŸ”´ Investigate & consider retraining (high average PSI)\"\n",
    "        print(f\"\\n{month} â€” {alert}\")\n",
    "        print(f\"Avg PSI: {avg_psi}\")\n",
    "        print(\"Drifted features (PSI > 0.1):\", drifted_feats)\n",
    "\n",
    "    elif avg_psi > 0.1:\n",
    "        alert = \"ðŸŸ¡ Monitor more closely (elevated average PSI)\"\n",
    "        print(f\"\\n{month} â€” {alert}\")\n",
    "        print(f\"Avg PSI: {avg_psi}\")\n",
    "        print(\"Drifted features (PSI > 0.1):\", drifted_feats)\n",
    "\n",
    "    elif max_psi > 0.1:\n",
    "        alert = \"ðŸŸ¡ Monitor individual features (some drift)\"\n",
    "        print(f\"\\n{month} â€” {alert}\")\n",
    "        print(f\"Max PSI: {max_psi}\")\n",
    "        print(\"Drifted features (PSI > 0.1):\", drifted_feats)\n",
    "\n",
    "    else:\n",
    "        alert = \"ðŸŸ¢ All clear (PSI stable)\"\n",
    "        print(f\"\\n{month} â€” {alert}\")\n",
    "\n",
    "    psi_results[month] = {\n",
    "        \"average_psi\": avg_psi,\n",
    "        \"max_psi\": max_psi,\n",
    "        \"drifted_features\": drifted_feats,\n",
    "        \"alert\": alert,\n",
    "        \"feature_psi\": valid_psi\n",
    "    }\n",
    "\n",
    "# Save one JSON file per month\n",
    "output_dir = \"datamart/gold/monitoring/stability\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for month, result in psi_results.items():\n",
    "    filename = f\"{output_dir}/stability_{month}.json\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(result, f, indent=2)\n",
    "\n",
    "print(\"\\nStability (PSI) monitoring complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aafef1e-5827-48e3-b5ba-3c29f3b17e5e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Monitor Performance (Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81b803b-9e2f-4479-b937-7c230f84f5e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load baseline test metrics (from model_store)\n",
    "with open(\"model_store/xgboost_metrics.json\") as f:\n",
    "    baseline = json.load(f)\n",
    "\n",
    "print(\"\\nBaseline (Test Set / February 2024) Metrics\")\n",
    "print(baseline)\n",
    "\n",
    "# Load labels\n",
    "labels_df = pd.read_parquet(\"data/gold/combined_labeled.parquet\")[[\"loan_id\", \"label\"]]\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(y_true, y_pred, y_prob):\n",
    "    return {\n",
    "        \"AUC\":       round(roc_auc_score(y_true, y_prob), 4),\n",
    "        \"Accuracy\":  round(accuracy_score(y_true, y_pred), 4),\n",
    "        \"Precision\": round(precision_score(y_true, y_pred), 4),\n",
    "        \"Recall\":    round(recall_score(y_true, y_pred), 4),\n",
    "        \"F1 Score\":  round(f1_score(y_true, y_pred), 4)\n",
    "    }\n",
    "\n",
    "# Percentage change calculator\n",
    "def percent_change(current, reference):\n",
    "    return round(((current - reference) / reference) * 100, 2)\n",
    "\n",
    "# Store results and alerts\n",
    "results = {}\n",
    "\n",
    "for month in [\"2024-03-01\", \"2024-04-01\", \"2024-05-01\", \"2024-06-01\"]:\n",
    "    path = f\"datamart/gold/predictions/predictions_oot_{month}.parquet\"\n",
    "    if os.path.exists(path):\n",
    "        preds_df = pd.read_parquet(path)\n",
    "        merged = preds_df.merge(labels_df, on=\"loan_id\", how=\"left\")\n",
    "\n",
    "        if merged[\"label\"].isnull().any():\n",
    "            print(f\"Warning: Missing labels in {month} predictions\")\n",
    "\n",
    "        y_true = merged[\"label\"].values\n",
    "        y_pred = merged[\"prediction\"].values\n",
    "        y_prob = merged[\"probability\"].values\n",
    "\n",
    "        # Evaluate metrics\n",
    "        current_metrics = evaluate(y_true, y_pred, y_prob)\n",
    "        print(f\"\\n=== {month} Metrics ===\")\n",
    "        print(current_metrics)\n",
    "\n",
    "        # Compare against baseline\n",
    "        changes = {k: percent_change(current_metrics[k], baseline[k]) for k in baseline}\n",
    "        dropped = {k: v for k, v in changes.items() if v < -5}\n",
    "        severe_drops = {k: v for k, v in changes.items() if v < -10}\n",
    "\n",
    "        # Determine alert level\n",
    "        if severe_drops:\n",
    "            alert = \"ðŸ”´ Significant performance drop\"\n",
    "        elif dropped:\n",
    "            alert = \"ðŸŸ¡ Moderate drop, monitor\"\n",
    "        else:\n",
    "            alert = \"ðŸŸ¢ All clear\"\n",
    "\n",
    "        print(f\"% Change vs Test Set: {changes}\")\n",
    "        print(f\"Alert: {alert}\")\n",
    "        if dropped:\n",
    "            print(\"Metrics with drops:\", dropped)\n",
    "\n",
    "        # Store\n",
    "        results[month] = {\n",
    "            \"metrics\": current_metrics,\n",
    "            \"percentage_change\": changes,\n",
    "            \"dropped_metrics\": dropped,\n",
    "            \"alert\": alert\n",
    "        }\n",
    "\n",
    "# Save one JSON file per month\n",
    "output_dir = \"datamart/gold/monitoring/performance\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for month, result in results.items():\n",
    "    filename = f\"{output_dir}/performance_{month}.json\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(result, f, indent=2)\n",
    "\n",
    "print(\"\\nPerformance monitoring complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d20f51-1fcc-44ea-bf32-52ecb0c297f7",
   "metadata": {},
   "source": [
    "### Visualise Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eceb50-71e6-4d62-be16-cf99df629e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load monthly PSI JSONs\n",
    "stability_dir = \"datamart/gold/monitoring/stability\"\n",
    "psi_data = {}\n",
    "\n",
    "for file in os.listdir(stability_dir):\n",
    "    if file.endswith(\".json\"):\n",
    "        month = file.replace(\"stability_\", \"\").replace(\".json\", \"\")\n",
    "        with open(os.path.join(stability_dir, file)) as f:\n",
    "            psi_data[month] = json.load(f)\n",
    "\n",
    "psi_df = pd.DataFrame.from_dict(\n",
    "    {month: data[\"average_psi\"] for month, data in psi_data.items()},\n",
    "    orient='index', columns=[\"Average PSI\"]\n",
    ").sort_index()\n",
    "psi_df.index.name = \"Month\"\n",
    "\n",
    "# Load monthly performance JSONs\n",
    "performance_dir = \"datamart/gold/monitoring/performance\"\n",
    "perf_data = {}\n",
    "\n",
    "for file in os.listdir(performance_dir):\n",
    "    if file.endswith(\".json\"):\n",
    "        month = file.replace(\"performance_\", \"\").replace(\".json\", \"\")\n",
    "        with open(os.path.join(performance_dir, file)) as f:\n",
    "            perf_data[month] = json.load(f)\n",
    "\n",
    "metrics = [\"AUC\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"]\n",
    "perf_df = pd.DataFrame.from_dict(\n",
    "    {month: data[\"metrics\"] for month, data in perf_data.items()},\n",
    "    orient='index'\n",
    ")[metrics].sort_index()\n",
    "perf_df.index.name = \"Month\"\n",
    "\n",
    "# Plotting\n",
    "output_dir = \"datamart/gold/monitoring\"\n",
    "\n",
    "# PSI Trend Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.lineplot(data=psi_df, x=psi_df.index, y=\"Average PSI\", marker=\"o\")\n",
    "plt.title(\"Stability: Average PSI Mar-Jun 2024\", fontsize=16)\n",
    "plt.ylabel(\"PSI\", fontsize=14)\n",
    "plt.xlabel(\"Month\", fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, \"psi_trend_mar_to_jun.png\"))\n",
    "plt.show()\n",
    "\n",
    "# Performance Metrics Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "for metric in metrics:\n",
    "    sns.lineplot(data=perf_df, x=perf_df.index, y=metric, marker=\"o\", label=metric)\n",
    "plt.title(\"Performance Metrics Mar-Jun 2024\", fontsize=16)\n",
    "plt.ylabel(\"Score\", fontsize=14)\n",
    "plt.xlabel(\"Month\", fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, \"performance_trends_mar_to_jun.png\"))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_asm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
